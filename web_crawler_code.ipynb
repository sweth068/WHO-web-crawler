{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11074fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pdfplumber\n",
    "import pubchempy as pcp\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "import time\n",
    "import pandas as pd\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210b0a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch the main page\n",
    "main_url = \"https://www.who.int/teams/health-product-and-policy-standards/inn/inn-lists\"\n",
    "response = requests.get(main_url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa13761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find relevant links\n",
    "links = soup.find_all('a', href=True)\n",
    "\n",
    "filtered_links = [link['href'] for link in links if 'inn-pl-' in link['href']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b5201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdn_links = []\n",
    "\n",
    "#Iterate over each link in filtered_links\n",
    "for link in filtered_links:\n",
    "    #Construct the full URL\n",
    "    full_url = link if link.startswith(\"http\") else f\"https://www.who.int{link}\"\n",
    "    \n",
    "    #print(f\"Accessing: {full_url}\")\n",
    "    \n",
    "    #Make a GET request\n",
    "    response = requests.get(full_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    #Find all <a> tags with href attributes\n",
    "    page_links = soup.find_all('a', href=True)\n",
    "    \n",
    "    #Filter links that start with the desired URL prefix\n",
    "    cdn_links += [a['href'] for a in page_links if a['href'].startswith(\"https://cdn.who.int/media/docs/default-source/international-nonproprietary-names-(inn)\")]\n",
    "\n",
    "#Print the collected links\n",
    "print(\"\\n Links starting with 'https://cdn.who.int/media/docs/default-source/international-nonproprietary-names-(inn)':\")\n",
    "print(cdn_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1cf9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directory for temporary files\n",
    "download_dir = \"temp_pdf\"\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "#Validate if the file is a proper PDF\n",
    "def is_valid_pdf(file_path):\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "#Extract chemical names with filtering logic\n",
    "def extract_filtered_chemical_names(pdf_path):\n",
    "    chemical_names = []\n",
    "    seen = set()\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if not text:\n",
    "                continue\n",
    "            for line in text.split('\\n'):\n",
    "                match = re.match(r'^([A-Za-záéíóúÁÉÍÓÚñÑüÜ-]+)', line.strip())\n",
    "                if match:\n",
    "                    name = match.group(1).strip()\n",
    "                    normalized = unicodedata.normalize('NFKD', name).encode('ASCII', 'ignore').decode('utf-8')\n",
    "                    #Apply filtering rules\n",
    "                    if (\n",
    "                        any(char.isupper() for char in normalized) or\n",
    "                        any(char.isdigit() for char in normalized) or\n",
    "                        normalized.endswith('-') or\n",
    "                        normalized.startswith('-') or\n",
    "                        len(normalized) <= 4\n",
    "                    ):\n",
    "                        continue\n",
    "                    if normalized not in seen:\n",
    "                        seen.add(normalized)\n",
    "                        chemical_names.append(normalized)\n",
    "\n",
    "    return sorted(chemical_names)\n",
    "\n",
    "#PubChem search and batch process\n",
    "def batch_process(names, batch_size=10, delay=5):\n",
    "    found_data = []\n",
    "    total_batches = (len(names) + batch_size - 1) // batch_size\n",
    "    for batch_num in range(total_batches):\n",
    "        start = batch_num * batch_size\n",
    "        end = start + batch_size\n",
    "        batch = names[start:end]\n",
    "        print(f\"\\n Processing batch {batch_num + 1}/{total_batches} ({len(batch)} items)...\")\n",
    "\n",
    "        for name in batch:\n",
    "            try:\n",
    "                results = pcp.get_compounds(name, 'name')\n",
    "                if results:\n",
    "                    compound = results[0]\n",
    "                    found_data.append({\n",
    "                        \"Name\": name,\n",
    "                        \"PubChem ID\": compound.cid,\n",
    "                        \"Molecular Formula\": compound.molecular_formula,\n",
    "                        \"Molecular Weight\": compound.molecular_weight,\n",
    "                        \"IUPAC Name\": compound.iupac_name\n",
    "                    })\n",
    "            except pcp.PubChemHTTPError as e:\n",
    "                if 'PUGREST.ServerBusy' in str(e):\n",
    "                    print(f\" Server busy for {name}, retrying...\")\n",
    "                    time.sleep(delay)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\" Failed to process {name}: {e}\")\n",
    "\n",
    "        print(f\" Waiting {delay} seconds before next batch...\")\n",
    "        time.sleep(delay)\n",
    "    return found_data\n",
    "\n",
    "#Iterate over each link in `cdn_links`\n",
    "for link in cdn_links:\n",
    "    #Extract the file name from the link\n",
    "    pdf_name = link.split(\"/\")[-1].split(\"?\")[0]\n",
    "    pdf_path = os.path.join(download_dir, pdf_name)\n",
    "\n",
    "    #Download the PDF\n",
    "    print(f\"\\n Downloading: {link}\")\n",
    "    response = requests.get(link)\n",
    "    with open(pdf_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "    #Validate the PDF\n",
    "    if not is_valid_pdf(pdf_path):\n",
    "        print(f\" File {pdf_name} is not a valid PDF. Skipping...\")\n",
    "        os.remove(pdf_path)\n",
    "        continue\n",
    "\n",
    "    print(f\"{pdf_name} is a valid PDF. Proceeding with processing...\")\n",
    "\n",
    "    #Extract chemical names\n",
    "    chemical_names = extract_filtered_chemical_names(pdf_path)\n",
    "    print(f\"Extracted chemicals count: {len(chemical_names)}\")\n",
    "\n",
    "    #Process chemical names in batches and save results\n",
    "    processed_data = batch_process(chemical_names, batch_size=50, delay=2)\n",
    "\n",
    "    #Save results to a CSV file named after the PDF\n",
    "    output_csv = f\"{pdf_name.split('.')[0]}_chemical_data.csv\"\n",
    "    pd.DataFrame(processed_data).to_csv(output_csv, index=False)\n",
    "    print(f\"Data saved to {output_csv}\")\n",
    "\n",
    "    #Delete the processed PDF file\n",
    "    os.remove(pdf_path)\n",
    "    print(f\"Deleted temporary file: {pdf_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
